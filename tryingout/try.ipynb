{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have to automate relu, by writing own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(train_images.reshape((n_train, w * h)), dtype=torch.float32)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_train = torch.tensor(train_labels, dtype=torch.long)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(test_images.reshape((n_test, w * h)), dtype=torch.float32)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_test = torch.tensor(test_labels, dtype=torch.long)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xb = torch.tensor(X[j * bs:(j + 1) * bs, :], dtype=torch.float32, requires_grad=False)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yb = torch.tensor(y[j * bs:(j + 1) * bs], dtype=torch.long, requires_grad=False)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000])\n",
      "torch.Size([10000, 784]) torch.Size([10000])\n",
      "input dim 784\n",
      "cross entropy loss  tensor(5.7968, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\3711410573.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)  # Corrected here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 1: 5.53, Testing accuracy: 7.48%\n",
      "cross entropy loss  tensor(5.5571, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 2: 5.29, Testing accuracy: 8.80%\n",
      "cross entropy loss  tensor(5.2262, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 3: 5.01, Testing accuracy: 13.59%\n",
      "cross entropy loss  tensor(4.9711, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 4: 4.68, Testing accuracy: 12.58%\n",
      "cross entropy loss  tensor(4.5427, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 5: 4.28, Testing accuracy: 15.68%\n",
      "cross entropy loss  tensor(4.2360, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 6: 3.85, Testing accuracy: 10.78%\n",
      "cross entropy loss  tensor(3.8373, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 7: 3.42, Testing accuracy: 19.35%\n",
      "cross entropy loss  tensor(3.3859, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 8: 3.06, Testing accuracy: 17.42%\n",
      "cross entropy loss  tensor(3.1919, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 9: 2.74, Testing accuracy: 19.07%\n",
      "cross entropy loss  tensor(2.7533, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 10: 2.47, Testing accuracy: 15.64%\n",
      "cross entropy loss  tensor(2.4424, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 11: 2.34, Testing accuracy: 17.55%\n",
      "cross entropy loss  tensor(2.3325, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 12: 2.39, Testing accuracy: 20.71%\n",
      "cross entropy loss  tensor(2.3796, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 13: 2.34, Testing accuracy: 23.78%\n",
      "cross entropy loss  tensor(2.2782, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 14: 2.37, Testing accuracy: 29.21%\n",
      "cross entropy loss  tensor(2.3495, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 15: 2.26, Testing accuracy: 15.10%\n",
      "cross entropy loss  tensor(2.1040, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 16: 2.31, Testing accuracy: 21.66%\n",
      "cross entropy loss  tensor(2.4486, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 17: 2.60, Testing accuracy: 11.79%\n",
      "cross entropy loss  tensor(2.7567, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 18: 2.93, Testing accuracy: 18.01%\n",
      "cross entropy loss  tensor(3.0560, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 19: 2.62, Testing accuracy: 30.45%\n",
      "cross entropy loss  tensor(2.4768, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 20: 2.31, Testing accuracy: 27.28%\n",
      "cross entropy loss  tensor(2.3110, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 21: 2.12, Testing accuracy: 28.85%\n",
      "cross entropy loss  tensor(2.1353, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 22: 2.07, Testing accuracy: 27.28%\n",
      "cross entropy loss  tensor(2.0554, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 23: 2.04, Testing accuracy: 33.72%\n",
      "cross entropy loss  tensor(2.1024, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 24: 2.16, Testing accuracy: 26.41%\n",
      "cross entropy loss  tensor(2.2351, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 25: 2.17, Testing accuracy: 26.50%\n",
      "cross entropy loss  tensor(1.9680, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 26: 2.06, Testing accuracy: 33.71%\n",
      "cross entropy loss  tensor(2.0578, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 27: 1.99, Testing accuracy: 37.63%\n",
      "cross entropy loss  tensor(1.8876, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 28: 1.86, Testing accuracy: 43.09%\n",
      "cross entropy loss  tensor(1.9471, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 29: 1.91, Testing accuracy: 42.46%\n",
      "cross entropy loss  tensor(1.8895, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 30: 1.86, Testing accuracy: 39.97%\n",
      "cross entropy loss  tensor(1.8007, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 31: 1.79, Testing accuracy: 43.95%\n",
      "cross entropy loss  tensor(1.7543, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 32: 1.83, Testing accuracy: 32.00%\n",
      "cross entropy loss  tensor(1.6554, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 33: 1.86, Testing accuracy: 34.08%\n",
      "cross entropy loss  tensor(1.7189, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 34: 2.07, Testing accuracy: 25.75%\n",
      "cross entropy loss  tensor(2.1712, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 35: 1.97, Testing accuracy: 40.06%\n",
      "cross entropy loss  tensor(2.0661, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 36: 1.75, Testing accuracy: 55.51%\n",
      "cross entropy loss  tensor(1.7661, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 37: 1.67, Testing accuracy: 57.44%\n",
      "cross entropy loss  tensor(1.6847, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 38: 1.73, Testing accuracy: 44.33%\n",
      "cross entropy loss  tensor(1.6355, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 39: 1.71, Testing accuracy: 39.93%\n",
      "cross entropy loss  tensor(1.5241, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 40: 1.56, Testing accuracy: 52.95%\n",
      "cross entropy loss  tensor(1.4237, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 41: 1.60, Testing accuracy: 47.10%\n",
      "cross entropy loss  tensor(1.4608, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 42: 1.54, Testing accuracy: 54.30%\n",
      "cross entropy loss  tensor(1.4716, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 43: 1.61, Testing accuracy: 41.72%\n",
      "cross entropy loss  tensor(1.4637, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 44: 1.91, Testing accuracy: 41.94%\n",
      "cross entropy loss  tensor(1.8977, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 45: 1.64, Testing accuracy: 37.28%\n",
      "cross entropy loss  tensor(1.5738, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 46: 1.51, Testing accuracy: 58.76%\n",
      "cross entropy loss  tensor(1.4883, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 47: 1.43, Testing accuracy: 54.46%\n",
      "cross entropy loss  tensor(1.6011, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 48: 1.66, Testing accuracy: 38.43%\n",
      "cross entropy loss  tensor(1.7498, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 49: 1.66, Testing accuracy: 49.33%\n",
      "cross entropy loss  tensor(1.8391, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 50: 1.47, Testing accuracy: 60.82%\n",
      "cross entropy loss  tensor(1.5651, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 51: 1.35, Testing accuracy: 63.64%\n",
      "cross entropy loss  tensor(1.3729, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 52: 1.31, Testing accuracy: 57.64%\n",
      "cross entropy loss  tensor(1.4493, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 53: 1.43, Testing accuracy: 56.52%\n",
      "cross entropy loss  tensor(1.3486, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 54: 1.41, Testing accuracy: 55.24%\n",
      "cross entropy loss  tensor(1.3342, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 55: 1.48, Testing accuracy: 48.20%\n",
      "cross entropy loss  tensor(1.5146, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 56: 1.43, Testing accuracy: 53.79%\n",
      "cross entropy loss  tensor(1.4134, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 57: 1.25, Testing accuracy: 65.15%\n",
      "cross entropy loss  tensor(1.3507, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 58: 1.30, Testing accuracy: 54.57%\n",
      "cross entropy loss  tensor(1.1415, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 59: 1.23, Testing accuracy: 63.74%\n",
      "cross entropy loss  tensor(1.2434, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 60: 1.29, Testing accuracy: 57.30%\n",
      "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.]])\n",
      "[9 2 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def manual_relu(x):\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "def load_mnist():\n",
    "    # Download MNIST dataset\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    # Extract data and labels\n",
    "    x_train, y_train = mnist_train.data, mnist_train.targets\n",
    "    x_test, y_test = mnist_test.data, mnist_test.targets\n",
    "\n",
    "    # Normalize data\n",
    "    x_train = (x_train.float() - 128.0) / 128.0\n",
    "    x_test = (x_test.float() - 128.0) / 128.0\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def calculate_loss(model, X, y):  \n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    \n",
    "    X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)  # Corrected here\n",
    "    \n",
    "    z1 = X.mm(W1) + b1\n",
    "    a1 = manual_relu(z1)\n",
    "    z2 = a1.mm(W2) + b2\n",
    "    a2 = manual_relu(z2)\n",
    "    z3 = a2.mm(W3) + b3\n",
    "    a3 = manual_relu(z3)\n",
    "    z4 = a3.mm(W4) + b4\n",
    "    a4 = manual_relu(z4)\n",
    "    z5 = a4.mm(W5) + b5\n",
    "    exp_scores = torch.exp(z5)\n",
    "    \n",
    "    probs = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
    "    \n",
    "    num_examples = X.shape[0]\n",
    "    corect_logprobs = -torch.log(probs[torch.arange(num_examples), y])\n",
    "    data_loss = torch.sum(corect_logprobs)\n",
    "    \n",
    "    return 1. / num_examples * data_loss.item()\n",
    "\n",
    "def predict(model, x):  \n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = \\\n",
    "        model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], \\\n",
    "        model['b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    \n",
    "    z1 = x.mm(W1) + b1\n",
    "    a1 = manual_relu(z1)\n",
    "    z2 = a1.mm(W2) + b2\n",
    "    a2 = manual_relu(z2)\n",
    "    z3 = a2.mm(W3) + b3\n",
    "    a3 = manual_relu(z3)\n",
    "    z4 = a3.mm(W4) + b4\n",
    "    a4 = manual_relu(z4)\n",
    "    z5 = a4.mm(W5) + b5\n",
    "    exp_scores = torch.exp(z5)\n",
    "    \n",
    "    probs = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
    "    \n",
    "    return torch.argmax(probs, dim=1).numpy()\n",
    "\n",
    "# Assuming the rest of the code for training and testing remains the same\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_model(X, y, nn_hdim, epsilon, reg_lambda, num_passes=60, print_loss=False):\n",
    "    torch.manual_seed(0)  \n",
    "    num_examples = X.shape[0]\n",
    "    nn_input_dim = nn_hdim[0]\n",
    "    print('input dim', nn_input_dim)\n",
    "\n",
    "    hdim1, hdim2, hdim3, hdim4, hdim5 = nn_hdim[1:]\n",
    "\n",
    "    W1 = torch.randn(nn_input_dim, hdim1, requires_grad=True) / (hdim1**0.5)\n",
    "    b1 = torch.zeros(1, hdim1, requires_grad=True)\n",
    "\n",
    "    W2 = torch.randn(hdim1, hdim2, requires_grad=True) / (hdim2**0.5)\n",
    "    b2 = torch.zeros(1, hdim2, requires_grad=True)\n",
    "\n",
    "    W3 = torch.randn(hdim2, hdim3, requires_grad=True) / (hdim3**0.5)\n",
    "    b3 = torch.zeros(1, hdim3, requires_grad=True)\n",
    "\n",
    "    W4 = torch.randn(hdim3, hdim4, requires_grad=True) / (hdim4**0.5)\n",
    "    b4 = torch.zeros(1, hdim4, requires_grad=True)\n",
    "\n",
    "    W5 = torch.randn(hdim4, hdim5, requires_grad=True) / (hdim5**0.5)\n",
    "    b5 = torch.zeros(1, hdim5, requires_grad=True)\n",
    "\n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,\n",
    "             'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4, 'W5': W5, 'b5': b5}\n",
    "\n",
    "    bs = 64  # Batch size\n",
    "    nbs_per_epoch = int(num_examples / bs)\n",
    "\n",
    "    for i in range(0, num_passes):\n",
    "        j = i % nbs_per_epoch\n",
    "        if j == 0:\n",
    "            ridx = torch.randperm(num_examples)\n",
    "            X = X[ridx, :]\n",
    "            y = y[ridx]\n",
    "\n",
    "        Xb = torch.tensor(X[j * bs:(j + 1) * bs, :], dtype=torch.float32, requires_grad=False)\n",
    "        yb = torch.tensor(y[j * bs:(j + 1) * bs], dtype=torch.long, requires_grad=False)\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = Xb.mm(W1) + b1\n",
    "        a1 = manual_relu(z1)  # Change tanh to manual_relu\n",
    "        z2 = a1.mm(W2) + b2\n",
    "        a2 = manual_relu(z2)  # Change tanh to manual_relu\n",
    "        z3 = a2.mm(W3) + b3\n",
    "        a3 = manual_relu(z3)  # Change tanh to manual_relu\n",
    "        z4 = a3.mm(W4) + b4\n",
    "        a4 = manual_relu(z4)  # Change tanh to manual_relu\n",
    "        z5 = a4.mm(W5) + b5\n",
    "        exp_scores = torch.exp(z5)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta_loss = exp_scores / exp_scores.sum(dim=1, keepdim=True)\n",
    "        delta_loss[torch.arange(bs), yb] -= 1\n",
    "        # Convert target labels to one-hot encoding\n",
    "        probs = exp_scores / exp_scores.sum(dim=1, keepdim=True)\n",
    "       \n",
    "        y_one_hot = torch.zeros_like(probs)\n",
    "        y_one_hot[torch.arange(yb.size(0)), yb] = 1\n",
    "\n",
    "        # Calculate cross-entropy loss manually\n",
    "        loss = -torch.sum(y_one_hot * torch.log(probs + 1e-8)) / probs.size(0)\n",
    "        print(\"cross entropy loss \", loss)\n",
    "\n",
    "        dW5 = a4.t().mm(delta_loss)\n",
    "        db5 = delta_loss.sum(dim=0, keepdim=True)\n",
    "        delta5 = delta_loss.mm(W5.t()) * (a4 > 0).float()  \n",
    "\n",
    "        dW4 = a3.t().mm(delta5)\n",
    "        db4 = delta5.sum(dim=0, keepdim=True)\n",
    "        delta4 = delta5.mm(W4.t()) * (a3 > 0).float() \n",
    "\n",
    "        dW3 = a2.t().mm(delta4)\n",
    "        db3 = delta4.sum(dim=0, keepdim=True)\n",
    "        delta3 = delta4.mm(W3.t()) * (a2 > 0).float() \n",
    "        dW2 = a1.t().mm(delta3)\n",
    "        db2 = delta3.sum(dim=0, keepdim=True)\n",
    "        delta2 = delta3.mm(W2.t()) * (a1 > 0).float() \n",
    "        dW1 = Xb.t().mm(delta2)\n",
    "        db1 = delta2.sum(dim=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        ## it is updating\n",
    "        \n",
    "        W1.data -= epsilon * dW1.data\n",
    "        b1.data -= epsilon * db1.data\n",
    "        W2.data -= epsilon * dW2.data\n",
    "        b2.data -= epsilon * db2.data\n",
    "        W3.data -= epsilon * dW3.data\n",
    "        b3.data -= epsilon * db3.data\n",
    "        W4.data -= epsilon * dW4.data\n",
    "        b4.data -= epsilon * db4.data\n",
    "        W5.data -= epsilon * dW5.data\n",
    "        b5.data -= epsilon * db5.data\n",
    "\n",
    "        # print(\"updated W1 \",W1.data  )\n",
    "        # print(\"updated b1 \",b1.data  )\n",
    "        # print(\"updated W2 \",W2.data  )\n",
    "        # print(\"updated b2 \",b2.data  )\n",
    "        # print(\"updated W3 \",W3.data  )\n",
    "        # print(\"updated b3 \",b3.data  )\n",
    "        # print(\"updated W4 \",W4.data  )\n",
    "        # print(\"updated b4 \",b4.data  )\n",
    "        # print(\"updated W4 \",W5.data  )\n",
    "        # print(\"updated b5 \",b5.data  )\n",
    "        # print(\"print loss value is \", print_loss)\n",
    "\n",
    "        y_pred = predict(model, X_test)\n",
    "        y_pred_tensor = torch.tensor(y_pred, dtype=torch.long)  # Convert y_pred to a tensor with dtype long\n",
    "        accuracy = torch.sum(y_pred_tensor == Y_test, dtype=torch.float32) / Y_test.shape[0]\n",
    "        print(\"Loss after iteration {}: {:.2f}, Testing accuracy: {:.2f}%\".format(i+1, calculate_loss(model, X, y), accuracy.item() * 100))\n",
    "\n",
    "    return model\n",
    "# load\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "# Assuming load_mnist, build_model, and predict functions are defined elsewhere\n",
    "\n",
    "(train_images, train_labels, test_images, test_labels) = load_mnist()\n",
    "n_train, w, h = train_images.shape\n",
    "X_train = torch.tensor(train_images.reshape((n_train, w * h)), dtype=torch.float32)\n",
    "Y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "n_test, w, h = test_images.shape\n",
    "X_test = torch.tensor(test_images.reshape((n_test, w * h)), dtype=torch.float32)\n",
    "Y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# Normalize data\n",
    "## main code check -> if done or not\n",
    "\n",
    "num_examples, input_dim = X_train.shape\n",
    "epsilon = 0.0003\n",
    "reg_lambda = 0.00\n",
    "\n",
    "# Assuming build_model and predict functions are defined accordingly for PyTorch\n",
    "hidden_size = 300\n",
    "model = build_model(X_train, Y_train, [input_dim, hidden_size , hidden_size, hidden_size, hidden_size, hidden_size], epsilon, reg_lambda, 60, print_loss=True)\n",
    "\n",
    "# Test output\n",
    "X_test0 = X_test[0:3, :]\n",
    "print(X_test0)\n",
    "y_pred0 = predict(model, X_test0)\n",
    "print(y_pred0)\n",
    "\n",
    "# # Reshape and plot images\n",
    "# X_test0 = X_test0.reshape(3, w, h).numpy()  # Convert PyTorch tensor to NumPy array for plotting\n",
    "# plt.figure('Predicted Image 1')\n",
    "# plt.imshow(X_test0[0, :, :])\n",
    "# plt.figure('Predicted Image 2')\n",
    "# plt.imshow(X_test0[1, :, :])\n",
    "# plt.figure('Predicted Image 3')\n",
    "# plt.imshow(X_test0[2, :, :])\n",
    "# pylab.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(train_images.reshape((n_train, w * h)), dtype=torch.float32)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_train = torch.tensor(train_labels, dtype=torch.long)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(test_images.reshape((n_test, w * h)), dtype=torch.float32)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_test = torch.tensor(test_labels, dtype=torch.long)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xb = torch.tensor(X[j * bs:(j + 1) * bs, :], dtype=torch.float32, requires_grad=False)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yb = torch.tensor(y[j * bs:(j + 1) * bs], dtype=torch.long, requires_grad=False)\n",
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000, 784]) torch.Size([42000])\n",
      "torch.Size([28000, 784]) torch.Size([28000])\n",
      "input dim 784\n",
      "cross entropy loss  tensor(5.7759, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chief Engineer (C)\\AppData\\Local\\Temp\\ipykernel_19100\\1462027679.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)  # Corrected here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 1: 5.53, Testing accuracy: 8.57%\n",
      "cross entropy loss  tensor(5.4181, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 2: 5.26, Testing accuracy: 12.19%\n",
      "cross entropy loss  tensor(5.4775, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 3: 4.99, Testing accuracy: 12.13%\n",
      "cross entropy loss  tensor(4.9884, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 4: 4.65, Testing accuracy: 12.38%\n",
      "cross entropy loss  tensor(4.6081, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 5: 4.25, Testing accuracy: 13.73%\n",
      "cross entropy loss  tensor(4.0860, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 6: 3.80, Testing accuracy: 17.44%\n",
      "cross entropy loss  tensor(3.8957, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 7: 3.36, Testing accuracy: 24.95%\n",
      "cross entropy loss  tensor(3.3186, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 8: 3.05, Testing accuracy: 18.44%\n",
      "cross entropy loss  tensor(2.9244, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 9: 2.83, Testing accuracy: 15.40%\n",
      "cross entropy loss  tensor(2.8977, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 10: 2.64, Testing accuracy: 19.31%\n",
      "cross entropy loss  tensor(2.6818, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 11: 2.53, Testing accuracy: 27.55%\n",
      "cross entropy loss  tensor(2.5990, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 12: 2.44, Testing accuracy: 21.44%\n",
      "cross entropy loss  tensor(2.3437, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 13: 2.29, Testing accuracy: 20.48%\n",
      "cross entropy loss  tensor(2.2576, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 14: 2.23, Testing accuracy: 21.49%\n",
      "cross entropy loss  tensor(2.2586, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 15: 2.25, Testing accuracy: 22.66%\n",
      "cross entropy loss  tensor(2.2622, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 16: 2.29, Testing accuracy: 21.55%\n",
      "cross entropy loss  tensor(2.3964, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 17: 2.17, Testing accuracy: 36.07%\n",
      "cross entropy loss  tensor(2.1441, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 18: 2.10, Testing accuracy: 26.68%\n",
      "cross entropy loss  tensor(2.1695, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 19: 2.74, Testing accuracy: 10.34%\n",
      "cross entropy loss  tensor(2.8308, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 20: 3.14, Testing accuracy: 15.56%\n",
      "cross entropy loss  tensor(3.2470, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 21: 2.76, Testing accuracy: 19.08%\n",
      "cross entropy loss  tensor(3.2380, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 22: 2.37, Testing accuracy: 29.52%\n",
      "cross entropy loss  tensor(2.2178, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 23: 2.28, Testing accuracy: 33.50%\n",
      "cross entropy loss  tensor(2.1002, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 24: 2.12, Testing accuracy: 38.66%\n",
      "cross entropy loss  tensor(2.3130, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 25: 2.08, Testing accuracy: 22.32%\n",
      "cross entropy loss  tensor(2.1070, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 26: 2.04, Testing accuracy: 40.18%\n",
      "cross entropy loss  tensor(2.2743, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 27: 2.04, Testing accuracy: 16.27%\n",
      "cross entropy loss  tensor(2.1062, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 28: 2.04, Testing accuracy: 29.55%\n",
      "cross entropy loss  tensor(2.1711, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 29: 2.04, Testing accuracy: 25.58%\n",
      "cross entropy loss  tensor(1.9895, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 30: 2.16, Testing accuracy: 26.77%\n",
      "cross entropy loss  tensor(1.9619, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 31: 1.86, Testing accuracy: 43.01%\n",
      "cross entropy loss  tensor(1.8015, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 32: 1.80, Testing accuracy: 36.30%\n",
      "cross entropy loss  tensor(1.8902, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 33: 1.89, Testing accuracy: 38.93%\n",
      "cross entropy loss  tensor(1.8696, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 34: 1.78, Testing accuracy: 45.67%\n",
      "cross entropy loss  tensor(1.6408, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 35: 1.89, Testing accuracy: 35.66%\n",
      "cross entropy loss  tensor(1.8037, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 36: 2.05, Testing accuracy: 29.79%\n",
      "cross entropy loss  tensor(1.9829, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 37: 1.77, Testing accuracy: 47.93%\n",
      "cross entropy loss  tensor(1.6959, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 38: 1.93, Testing accuracy: 29.89%\n",
      "cross entropy loss  tensor(2.0098, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 39: 1.67, Testing accuracy: 54.41%\n",
      "cross entropy loss  tensor(1.7117, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 40: 1.55, Testing accuracy: 59.43%\n",
      "cross entropy loss  tensor(1.6486, grad_fn=<DivBackward0>)\n",
      "Loss after iteration 41: 1.58, Testing accuracy: 55.24%\n",
      "cross entropy loss  tensor(1.6489, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def manual_relu(x):\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_mnist():\n",
    "    # Download MNIST dataset\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    # Extract data and labels\n",
    "    x_train, y_train = mnist_train.data, mnist_train.targets\n",
    "    x_test, y_test = mnist_test.data, mnist_test.targets\n",
    "\n",
    "    # Normalize data\n",
    "    x_train = (x_train.float() - 128.0) / 128.0\n",
    "    x_test = (x_test.float() - 128.0) / 128.0\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        torch.cat([x_train.unsqueeze(1), x_test.unsqueeze(1)]),  # Add a channel dimension\n",
    "        torch.cat([y_train, y_test]),\n",
    "        test_size=0.4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return x_train.squeeze(1), y_train, x_test.squeeze(1), y_test\n",
    "\n",
    "def calculate_loss(model, X, y):  \n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    \n",
    "    X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)  # Corrected here\n",
    "    \n",
    "    z1 = X.mm(W1) + b1\n",
    "    a1 = manual_relu(z1)\n",
    "    z2 = a1.mm(W2) + b2\n",
    "    a2 = manual_relu(z2)\n",
    "    z3 = a2.mm(W3) + b3\n",
    "    a3 = manual_relu(z3)\n",
    "    z4 = a3.mm(W4) + b4\n",
    "    a4 = manual_relu(z4)\n",
    "    z5 = a4.mm(W5) + b5\n",
    "    exp_scores = torch.exp(z5)\n",
    "    \n",
    "    probs = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
    "    \n",
    "    num_examples = X.shape[0]\n",
    "    corect_logprobs = -torch.log(probs[torch.arange(num_examples), y])\n",
    "    data_loss = torch.sum(corect_logprobs)\n",
    "    \n",
    "    return 1. / num_examples * data_loss.item()\n",
    "\n",
    "def predict(model, x):  \n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = \\\n",
    "        model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], \\\n",
    "        model['b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    \n",
    "    z1 = x.mm(W1) + b1\n",
    "    a1 = manual_relu(z1)\n",
    "    z2 = a1.mm(W2) + b2\n",
    "    a2 = manual_relu(z2)\n",
    "    z3 = a2.mm(W3) + b3\n",
    "    a3 = manual_relu(z3)\n",
    "    z4 = a3.mm(W4) + b4\n",
    "    a4 = manual_relu(z4)\n",
    "    z5 = a4.mm(W5) + b5\n",
    "    exp_scores = torch.exp(z5)\n",
    "    \n",
    "    probs = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
    "    \n",
    "    return torch.argmax(probs, dim=1).numpy()\n",
    "\n",
    "# Assuming the rest of the code for training and testing remains the same\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_model(X, y, nn_hdim, epsilon, reg_lambda, num_passes=60, print_loss=False):\n",
    "    torch.manual_seed(0)  \n",
    "    num_examples = X.shape[0]\n",
    "    nn_input_dim = nn_hdim[0]\n",
    "    print('input dim', nn_input_dim)\n",
    "\n",
    "    hdim1, hdim2, hdim3, hdim4, hdim5 = nn_hdim[1:]\n",
    "\n",
    "    W1 = torch.randn(nn_input_dim, hdim1, requires_grad=True) / (hdim1**0.5)\n",
    "    b1 = torch.zeros(1, hdim1, requires_grad=True)\n",
    "\n",
    "    W2 = torch.randn(hdim1, hdim2, requires_grad=True) / (hdim2**0.5)\n",
    "    b2 = torch.zeros(1, hdim2, requires_grad=True)\n",
    "\n",
    "    W3 = torch.randn(hdim2, hdim3, requires_grad=True) / (hdim3**0.5)\n",
    "    b3 = torch.zeros(1, hdim3, requires_grad=True)\n",
    "\n",
    "    W4 = torch.randn(hdim3, hdim4, requires_grad=True) / (hdim4**0.5)\n",
    "    b4 = torch.zeros(1, hdim4, requires_grad=True)\n",
    "\n",
    "    W5 = torch.randn(hdim4, hdim5, requires_grad=True) / (hdim5**0.5)\n",
    "    b5 = torch.zeros(1, hdim5, requires_grad=True)\n",
    "\n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,\n",
    "             'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4, 'W5': W5, 'b5': b5}\n",
    "\n",
    "    bs = 64  # Batch size\n",
    "    nbs_per_epoch = int(num_examples / bs)\n",
    "\n",
    "    for i in range(0, num_passes):\n",
    "        j = i % nbs_per_epoch\n",
    "        if j == 0:\n",
    "            ridx = torch.randperm(num_examples)\n",
    "            X = X[ridx, :]\n",
    "            y = y[ridx]\n",
    "\n",
    "        Xb = torch.tensor(X[j * bs:(j + 1) * bs, :], dtype=torch.float32, requires_grad=False)\n",
    "        yb = torch.tensor(y[j * bs:(j + 1) * bs], dtype=torch.long, requires_grad=False)\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = Xb.mm(W1) + b1\n",
    "        a1 = manual_relu(z1)  # Change tanh to manual_relu\n",
    "        z2 = a1.mm(W2) + b2\n",
    "        a2 = manual_relu(z2)  # Change tanh to manual_relu\n",
    "        z3 = a2.mm(W3) + b3\n",
    "        a3 = manual_relu(z3)  # Change tanh to manual_relu\n",
    "        z4 = a3.mm(W4) + b4\n",
    "        a4 = manual_relu(z4)  # Change tanh to manual_relu\n",
    "        z5 = a4.mm(W5) + b5\n",
    "        exp_scores = torch.exp(z5)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta_loss = exp_scores / exp_scores.sum(dim=1, keepdim=True)\n",
    "        delta_loss[torch.arange(bs), yb] -= 1\n",
    "        # Convert target labels to one-hot encoding\n",
    "        probs = exp_scores / exp_scores.sum(dim=1, keepdim=True)\n",
    "       \n",
    "        y_one_hot = torch.zeros_like(probs)\n",
    "        y_one_hot[torch.arange(yb.size(0)), yb] = 1\n",
    "\n",
    "        # Calculate cross-entropy loss manually\n",
    "        loss = -torch.sum(y_one_hot * torch.log(probs + 1e-8)) / probs.size(0)\n",
    "        print(\"cross entropy loss \", loss)\n",
    "\n",
    "        dW5 = a4.t().mm(delta_loss)\n",
    "        db5 = delta_loss.sum(dim=0, keepdim=True)\n",
    "        delta5 = delta_loss.mm(W5.t()) * (a4 > 0).float()  \n",
    "\n",
    "        dW4 = a3.t().mm(delta5)\n",
    "        db4 = delta5.sum(dim=0, keepdim=True)\n",
    "        delta4 = delta5.mm(W4.t()) * (a3 > 0).float() \n",
    "\n",
    "        dW3 = a2.t().mm(delta4)\n",
    "        db3 = delta4.sum(dim=0, keepdim=True)\n",
    "        delta3 = delta4.mm(W3.t()) * (a2 > 0).float() \n",
    "        dW2 = a1.t().mm(delta3)\n",
    "        db2 = delta3.sum(dim=0, keepdim=True)\n",
    "        delta2 = delta3.mm(W2.t()) * (a1 > 0).float() \n",
    "        dW1 = Xb.t().mm(delta2)\n",
    "        db1 = delta2.sum(dim=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        ## it is updating\n",
    "        \n",
    "        W1.data -= epsilon * dW1.data\n",
    "        b1.data -= epsilon * db1.data\n",
    "        W2.data -= epsilon * dW2.data\n",
    "        b2.data -= epsilon * db2.data\n",
    "        W3.data -= epsilon * dW3.data\n",
    "        b3.data -= epsilon * db3.data\n",
    "        W4.data -= epsilon * dW4.data\n",
    "        b4.data -= epsilon * db4.data\n",
    "        W5.data -= epsilon * dW5.data\n",
    "        b5.data -= epsilon * db5.data\n",
    "\n",
    "        # print(\"updated W1 \",W1.data  )\n",
    "        # print(\"updated b1 \",b1.data  )\n",
    "        # print(\"updated W2 \",W2.data  )\n",
    "        # print(\"updated b2 \",b2.data  )\n",
    "        # print(\"updated W3 \",W3.data  )\n",
    "        # print(\"updated b3 \",b3.data  )\n",
    "        # print(\"updated W4 \",W4.data  )\n",
    "        # print(\"updated b4 \",b4.data  )\n",
    "        # print(\"updated W4 \",W5.data  )\n",
    "        # print(\"updated b5 \",b5.data  )\n",
    "        # print(\"print loss value is \", print_loss)\n",
    "\n",
    "        y_pred = predict(model, X_test)\n",
    "        y_pred_tensor = torch.tensor(y_pred, dtype=torch.long)  # Convert y_pred to a tensor with dtype long\n",
    "        accuracy = torch.sum(y_pred_tensor == Y_test, dtype=torch.float32) / Y_test.shape[0]\n",
    "        print(\"Loss after iteration {}: {:.2f}, Testing accuracy: {:.2f}%\".format(i+1, calculate_loss(model, X, y), accuracy.item() * 100))\n",
    "\n",
    "    return model\n",
    "# load\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "# Assuming load_mnist, build_model, and predict functions are defined elsewhere\n",
    "\n",
    "(train_images, train_labels, test_images, test_labels) = load_mnist()\n",
    "n_train, w, h = train_images.shape\n",
    "X_train = torch.tensor(train_images.reshape((n_train, w * h)), dtype=torch.float32)\n",
    "Y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "n_test, w, h = test_images.shape\n",
    "X_test = torch.tensor(test_images.reshape((n_test, w * h)), dtype=torch.float32)\n",
    "Y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# Normalize data\n",
    "## main code check -> if done or not\n",
    "\n",
    "num_examples, input_dim = X_train.shape\n",
    "epsilon = 0.0003\n",
    "reg_lambda = 0.00\n",
    "\n",
    "# Assuming build_model and predict functions are defined accordingly for PyTorch\n",
    "hidden_size = 300\n",
    "model = build_model(X_train, Y_train, [input_dim, hidden_size , hidden_size, hidden_size, hidden_size, hidden_size], epsilon, reg_lambda, 60, print_loss=True)\n",
    "\n",
    "# Test output\n",
    "X_test0 = X_test[0:3, :]\n",
    "print(X_test0)\n",
    "y_pred0 = predict(model, X_test0)\n",
    "print(y_pred0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
