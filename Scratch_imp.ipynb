{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x64 and 32x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 99\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    101\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[7], line 56\u001b[0m, in \u001b[0;36mcustom_backward\u001b[1;34m(model, inputs, labels, learning_rate)\u001b[0m\n\u001b[0;32m     53\u001b[0m grad_logits \u001b[38;5;241m=\u001b[39m (logits \u001b[38;5;241m-\u001b[39m grad_output) \u001b[38;5;241m/\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Backward pass through fully connected layers\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m grad_fc5 \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m grad_relu4 \u001b[38;5;241m=\u001b[39m (grad_logits \u001b[38;5;241m@\u001b[39m model\u001b[38;5;241m.\u001b[39mfc5\u001b[38;5;241m.\u001b[39mweight) \u001b[38;5;241m*\u001b[39m (model\u001b[38;5;241m.\u001b[39mrelu4(model\u001b[38;5;241m.\u001b[39mfc4\u001b[38;5;241m.\u001b[39mweight) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Adjust shape for matrix multiplication\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x64 and 32x32)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "# Define the custom neural network architecture\n",
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CustomNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Custom dataset, DataLoader, and training loop are assumed to be available from the previous question.\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 28 * 28  # Assuming input size for MNIST-like data\n",
    "hidden_size = 32\n",
    "num_classes = 10\n",
    "learning_rate = 0.0003\n",
    "epochs = 60\n",
    "\n",
    "model = CustomNeuralNetwork(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Custom backpropagation implementation\n",
    "def custom_backward(model, inputs, labels, learning_rate):\n",
    "    # Forward pass\n",
    "    logits = model(inputs)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Calculate gradients manually\n",
    "    grad_output = (logits.argmax(dim=1) == labels).float().view(-1, 1)\n",
    "    grad_logits = (logits - grad_output) / inputs.size(0)\n",
    "\n",
    "    # Backward pass through fully connected layers\n",
    "    grad_fc5 = grad_logits.t() @ model.relu4(model.fc4.weight).t()\n",
    "    grad_relu4 = (grad_logits @ model.fc5.weight) * (model.relu4(model.fc4.weight) > 0).float()\n",
    "\n",
    "    # Adjust shape for matrix multiplication\n",
    "    grad_fc4 = grad_relu4.t() @ model.relu3(model.fc3.weight).t()\n",
    "    grad_relu3 = (grad_relu4 @ model.fc4.weight) * (model.relu3(model.fc3.weight) > 0).float()\n",
    "\n",
    "    grad_fc3 = grad_relu3.t() @ model.relu2(model.fc2.weight).t()\n",
    "    grad_relu2 = (grad_relu3 @ model.fc3.weight) * (model.relu2(model.fc2.weight) > 0).float()\n",
    "\n",
    "    grad_fc2 = grad_relu2.t() @ model.relu1(model.fc1.weight).t()\n",
    "    grad_relu1 = (grad_relu2 @ model.fc2.weight) * (model.relu1(model.fc1.weight) > 0).float()\n",
    "\n",
    "    # Update weights\n",
    "    model.fc1.weight.data -= learning_rate * grad_fc2\n",
    "    model.fc2.weight.data -= learning_rate * grad_fc2\n",
    "    model.fc3.weight.data -= learning_rate * grad_fc3\n",
    "    model.fc4.weight.data -= learning_rate * grad_fc4\n",
    "    model.fc5.weight.data -= learning_rate * grad_logits.t() @ model.fc5.weight\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Training loop\n",
    "train_losses, val_losses, test_losses = [], [], []\n",
    "train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the dataset and data loaders\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Assuming you have datasets named train_dataset, val_dataset, and test_dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size= 64, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = custom_backward(model, inputs.view(inputs.size(0), -1), labels, learning_rate)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_accuracy = evaluate_model(model, train_loader, criterion)\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, '\n",
    "              f'Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, '\n",
    "              f'Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Plotting the graphs\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss graphs\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation')\n",
    "plt.plot(range(1, epochs + 1), test_losses, label='Testing')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy graphs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Training')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation')\n",
    "plt.plot(range(1, epochs + 1), test_accuracies, label='Testing')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 148\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss after iteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, testing accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    143\u001b[0m                   \u001b[38;5;28mformat\u001b[39m(i, calculate_loss(model, X, y), accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 148\u001b[0m (train_images, train_labels, test_images, test_labels) \u001b[38;5;241m=\u001b[39m \u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    149\u001b[0m n_train, w, h \u001b[38;5;241m=\u001b[39m train_images\u001b[38;5;241m.\u001b[39mshape  \n\u001b[0;32m    150\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train_images\u001b[38;5;241m.\u001b[39mreshape((n_train, w \u001b[38;5;241m*\u001b[39m h))  \n",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36mload_mnist\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_mnist\u001b[39m():\n\u001b[0;32m      6\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist.npz\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[1;32m----> 7\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     x_train, y_train \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m'\u001b[39m], f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m     x_test, y_test \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_test\u001b[39m\u001b[38;5;124m'\u001b[39m], f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Chief Engineer (C)\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist.npz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "def load_mnist():\n",
    "    path = 'mnist.npz'  \n",
    "    f = np.load(path)\n",
    "    x_train, y_train = f['x_train'], f['y_train']\n",
    "    x_test, y_test = f['x_test'], f['y_test']\n",
    "    f.close()\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "def calculate_loss(model, X, y): \n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model[\n",
    "        'b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    z1 = X.dot(W1) + b1  \n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(W3) + b3 \n",
    "    a3 = np.tanh(z3)\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    a4 = np.tanh(z4)\n",
    "    z5 = a4.dot(W5) + b5\n",
    "    exp_scores = np.exp(z5)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    num_examples = X.shape[0]\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # data_loss += reg_lambda / 2 * (\n",
    "    #         np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) + np.sum(np.square(W4)))\n",
    "    return 1. / num_examples * data_loss\n",
    "\n",
    "\n",
    "def predict(model, x):  # Forward\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = \\\n",
    "        model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], \\\n",
    "        model['b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    a3 = np.tanh(z3)\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    a4 = np.tanh(z4)\n",
    "    z5 = a4.dot(W5) + b5\n",
    "    exp_scores = np.exp(z5)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1) \n",
    "\n",
    "def build_model(X, y, nn_hdim, epsilon, reg_lambda, num_passes=60, print_loss=False):\n",
    "    np.random.seed(0)  \n",
    "    num_examples = X.shape[0]\n",
    "    nn_input_dim = nn_hdim[0]\n",
    "    print('input dim', nn_input_dim)\n",
    "\n",
    "    hdim1 = nn_hdim[1]\n",
    "    W1 = np.random.randn(nn_input_dim, hdim1) / np.sqrt(hdim1)\n",
    "    b1 = np.zeros((1, hdim1)) \n",
    "    print('fc: %d -> %d' % (nn_input_dim, hdim1))\n",
    "    hdim2 = nn_hdim[2]\n",
    "    W2 = np.random.randn(hdim1, hdim2) / np.sqrt(hdim2)\n",
    "    b2 = np.zeros((1, hdim2))\n",
    "    print('fc: %d -> %d' % (hdim1, hdim2))\n",
    "    hdim3 = nn_hdim[3]\n",
    "    W3 = np.random.randn(hdim2, hdim3) / np.sqrt(hdim3)\n",
    "    b3 = np.zeros((1, hdim3))\n",
    "    print('fc: %d -> %d' % (hdim2, hdim3))\n",
    "    hdim4 = nn_hdim[4]\n",
    "    W4 = np.random.randn(hdim3, hdim4) / np.sqrt(hdim4)\n",
    "    b4 = np.zeros((1, hdim4))\n",
    "    print('fc: %d -> %d' % (hdim3, hdim4))\n",
    "    hdim5 = nn_hdim[5]\n",
    "    W5 = np.random.randn(hdim4, hdim5) / np.sqrt(hdim5)\n",
    "    b5 = np.zeros((1, hdim5))\n",
    "    print('fc: %d -> %d' % (hdim4, hdim5))\n",
    "\n",
    "    # train：\n",
    "    model = {}\n",
    "    bs = 128 #batchsize\n",
    "    nbs_per_epoch = int(num_examples / bs)\n",
    "    for i in range(0, num_passes):\n",
    "        j = i % nbs_per_epoch\n",
    "        if 0 == j:\n",
    "            ridx = np.asarray(list(range(num_examples)))\n",
    "            np.random.shuffle(ridx)\n",
    "            X = X[ridx, :]\n",
    "            y = y[ridx]\n",
    "        Xb = X[j * bs:(j + 1) * bs, :]\n",
    "        yb = y[j * bs:(j + 1) * bs]\n",
    "        # Forward propagation\n",
    "        z1 = Xb.dot(W1) + b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU activation\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        a2 = np.maximum(0, z2)  # ReLU activation\n",
    "        z3 = a2.dot(W3) + b3\n",
    "        a3 = np.maximum(0, z3)  # ReLU activation\n",
    "        z4 = a3.dot(W4) + b4\n",
    "        a4 = np.maximum(0, z4)  # ReLU activation\n",
    "        z5 = a4.dot(W5) + b5\n",
    "        exp_scores = np.exp(z5)\n",
    "        # Backpropagation\n",
    "        delta_loss = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        delta_loss[range(bs), yb] -= 1\n",
    "        dW5 = (a4.T).dot(delta_loss)\n",
    "        db5 = np.sum(delta_loss, axis=0, keepdims=True)\n",
    "        delta5 = delta_loss.dot(W5.T) * (a4 > 0)  # Derivative of ReLU\n",
    "        dW4 = (a3.T).dot(delta5)\n",
    "        db4 = np.sum(delta5, axis=0, keepdims=True)\n",
    "        delta4 = delta5.dot(W4.T) * (a3 > 0)  # Derivative of ReLU\n",
    "        dW3 = (a2.T).dot(delta4)\n",
    "        db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(W3.T) * (a2 > 0)  # Derivative of ReLU\n",
    "        dW2 = a1.T.dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (a1 > 0)  # Derivative of ReLU\n",
    "        dW1 = (Xb.T).dot(delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        # dW5 += reg_lambda * W5\n",
    "        # dW4 += reg_lambda * W4\n",
    "        # dW3 += reg_lambda * W3\n",
    "        # dW2 += reg_lambda * W2\n",
    "        # dW1 += reg_lambda * W1\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        W3 += -epsilon * dW3\n",
    "        b3 += -epsilon * db3\n",
    "        W4 += -epsilon * dW4\n",
    "        b4 += -epsilon * db4\n",
    "        W5 += -epsilon * dW5\n",
    "        b5 += -epsilon * db5\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,\n",
    "                 'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4, 'W5': W5, 'b5': b5}\n",
    "\n",
    "        if print_loss and i % 2000 == 0:\n",
    "            epsilon *= 0.99\n",
    "            y_pred = predict(model, X_test)\n",
    "            accuracy = sum(0 == (y_pred - Y_test)) / Y_test.shape[0]\n",
    "            print(\"loss after iteration {}: {:.2f}, testing accuracy: {:.2f}%\".\n",
    "                  format(i, calculate_loss(model, X, y), accuracy * 100))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "(train_images, train_labels, test_images, test_labels) = load_mnist() \n",
    "n_train, w, h = train_images.shape  \n",
    "X_train = train_images.reshape((n_train, w * h))  \n",
    "Y_train = train_labels \n",
    "n_test, w, h = test_images.shape\n",
    "X_test = test_images.reshape((n_test, w * h))\n",
    "Y_test = test_labels\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "# train\n",
    "X_train = (X_train.astype(float) - 128.0) / 128.0  \n",
    "X_test = (X_test.astype(float) - 128.0) / 128.0\n",
    "num_examples, input_dim = X_train.shape\n",
    "epsilon = 0.0003\n",
    "reg_lambda = 0.00\n",
    "model = build_model(X_train, Y_train, [input_dim, 256, 128, 63, 32, 10], epsilon, reg_lambda, 60, print_loss=True)\n",
    "# test output\n",
    "X_test0=X_test[0:3,:]\n",
    "y_pred0 = predict(model, X_test0)\n",
    "print(y_pred0)\n",
    "X_test0=X_test0.reshape(3,w,h)\n",
    "plt.figure('第一张图预测')\n",
    "plt.imshow(X_test0[0,:,:])\n",
    "plt.figure('第二张图预测')\n",
    "plt.imshow(X_test0[1,:,:])\n",
    "plt.figure('第三张图预测')\n",
    "plt.imshow(X_test0[2,:,:])\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x10 and 40x40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 78\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     77\u001b[0m d_loss_dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Gradient of the loss with respect to the final output\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc5.weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss_dy \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mrelu4(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc4\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag_embed(torch\u001b[38;5;241m.\u001b[39mones_like(model\u001b[38;5;241m.\u001b[39mfc5\u001b[38;5;241m.\u001b[39mweight))\n\u001b[0;32m     79\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc5.bias\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss_dy \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(model\u001b[38;5;241m.\u001b[39mfc5\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m     81\u001b[0m d_loss_dz4 \u001b[38;5;241m=\u001b[39m d_loss_dy \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mfc5(outputs)\u001b[38;5;241m.\u001b[39mmm(torch\u001b[38;5;241m.\u001b[39mdiag_embed(model\u001b[38;5;241m.\u001b[39mfc5\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()))\n",
      "File \u001b[1;32mc:\\Users\\Chief Engineer (C)\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Chief Engineer (C)\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x10 and 40x40)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the neural network architecture\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Define the dataset and data loaders\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 28 * 28\n",
    "hidden_size = 40\n",
    "output_size = 10\n",
    "model = FeedForwardNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0003)\n",
    "num_epochs = 60\n",
    "train_losses, val_losses, test_losses = [], [], []\n",
    "train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct_train = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Manual backpropagation\n",
    "        grads = {}\n",
    "        grads['fc1.weight'] = torch.zeros_like(model.fc1.weight)\n",
    "        grads['fc1.bias'] = torch.zeros_like(model.fc1.bias)\n",
    "        grads['fc2.weight'] = torch.zeros_like(model.fc2.weight)\n",
    "        grads['fc2.bias'] = torch.zeros_like(model.fc2.bias)\n",
    "        grads['fc3.weight'] = torch.zeros_like(model.fc3.weight)\n",
    "        grads['fc3.bias'] = torch.zeros_like(model.fc3.bias)\n",
    "        grads['fc4.weight'] = torch.zeros_like(model.fc4.weight)\n",
    "        grads['fc4.bias'] = torch.zeros_like(model.fc4.bias)\n",
    "        grads['fc5.weight'] = torch.zeros_like(model.fc5.weight)\n",
    "        grads['fc5.bias'] = torch.zeros_like(model.fc5.bias)\n",
    "\n",
    "        # Backward pass\n",
    "        # Backward pass\n",
    "        # Backward pass\n",
    "        d_loss_dy = 1.0  # Gradient of the loss with respect to the final output\n",
    "        grads['fc5.weight'] += d_loss_dy * model.relu4(model.fc4(outputs)).t() @ torch.diag_embed(torch.ones_like(model.fc5.weight))\n",
    "        grads['fc5.bias'] += d_loss_dy * torch.ones_like(model.fc5.bias)\n",
    "\n",
    "        d_loss_dz4 = d_loss_dy * model.fc5(outputs).mm(torch.diag_embed(model.fc5.weight.t()))\n",
    "        d_loss_dx4 = d_loss_dz4 * (outputs > 0).float()\n",
    "        grads['fc4.weight'] += model.relu4(model.fc4(outputs)).t() @ d_loss_dx4\n",
    "        grads['fc4.bias'] += d_loss_dx4.sum(dim=0)\n",
    "\n",
    "        d_loss_dz3 = d_loss_dx4 @ model.fc4.weight.t()\n",
    "        d_loss_dx3 = d_loss_dz3 * (model.fc4(outputs) > 0).float()\n",
    "        grads['fc3.weight'] += model.relu3(model.fc3(inputs)).t() @ d_loss_dx3\n",
    "        grads['fc3.bias'] += d_loss_dx3.sum(dim=0)\n",
    "\n",
    "        d_loss_dz2 = d_loss_dx3 @ model.fc3.weight.t()\n",
    "        d_loss_dx2 = d_loss_dz2 * (model.fc3(inputs) > 0).float()\n",
    "        grads['fc2.weight'] += model.relu2(model.fc2(inputs)).t() @ d_loss_dx2\n",
    "        grads['fc2.bias'] += d_loss_dx2.sum(dim=0)\n",
    "\n",
    "        d_loss_dz1 = d_loss_dx2 @ model.fc2.weight.t()\n",
    "        d_loss_dx1 = d_loss_dz1 * (model.fc2(inputs) > 0).float()\n",
    "        grads['fc1.weight'] += inputs.t() @ d_loss_dx1\n",
    "        grads['fc1.bias'] += d_loss_dx1.sum(dim=0)\n",
    "# Update parameters manually\n",
    "with torch.no_grad():\n",
    "    for param_name, param in model.named_parameters():\n",
    "        param -= 0.0003 * grads[param_name]  # Using the learning rate of 0.0003\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, correct_val = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_accuracy = correct_val / len(test_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss, correct_test = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = correct_test / len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plotting graphs\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(val_accuracies, label='Validation')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
